{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ce25ba",
   "metadata": {},
   "source": [
    "## Analyse Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28adcec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m text = \u001b[38;5;28mopen\u001b[39m(os.path.join(data_path, f), \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m).read()\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Extraction des mots-clés du fichier\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m keywords = \u001b[43mkw_extractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m kw, score \u001b[38;5;129;01min\u001b[39;00m keywords:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Ne garder que les bigrammes\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kw.split()) == \u001b[32m2\u001b[39m:\n\u001b[32m     23\u001b[39m         \u001b[38;5;66;03m# YAKE : plus le score est petit, mieux c’est → on garde le meilleur\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\core\\yake.py:213\u001b[39m, in \u001b[36mKeywordExtractor.extract_keywords\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    207\u001b[39m core_config = {\n\u001b[32m    208\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwindows_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mwindow_size\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    209\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    210\u001b[39m }\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Initialize the data core with the text\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m dc = \u001b[43mDataCore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopword_set\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstopword_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcore_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Build features for single terms and multi-word terms\u001b[39;00m\n\u001b[32m    216\u001b[39m dc.build_single_terms_features(features=\u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\core.py:86\u001b[39m, in \u001b[36mDataCore.__init__\u001b[39m\u001b[34m(self, text, stopword_set, config)\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mself\u001b[39m._state[\u001b[33m\"\u001b[39m\u001b[33mcollections\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mfreq_ns\u001b[39m\u001b[33m\"\u001b[39m][i + \u001b[32m1\u001b[39m] = \u001b[32m0.0\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Process the text and build all data structures\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindows_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\core.py:191\u001b[39m, in \u001b[36mDataCore._build\u001b[39m\u001b[34m(self, text, windows_size, n)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# Process each sentence individually\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentence_id, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.sentences_str):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     pos_text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Store the total number of processed words\u001b[39;00m\n\u001b[32m    194\u001b[39m \u001b[38;5;28mself\u001b[39m.number_of_words = pos_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\core.py:237\u001b[39m, in \u001b[36mDataCore._process_sentence\u001b[39m\u001b[34m(self, sentence, sentence_id, pos_text, context)\u001b[39m\n\u001b[32m    232\u001b[39m         word_context = {\n\u001b[32m    233\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpos_sent\u001b[39m\u001b[33m\"\u001b[39m: pos_sent,  \u001b[38;5;66;03m# Position within the sentence\u001b[39;00m\n\u001b[32m    234\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mblock_of_word_obj\u001b[39m\u001b[33m\"\u001b[39m: block_of_word_obj,  \u001b[38;5;66;03m# Current word block\u001b[39;00m\n\u001b[32m    235\u001b[39m         }\n\u001b[32m    236\u001b[39m         \u001b[38;5;66;03m# Process this word and update position counter\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m         pos_text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_word\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m            \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_context\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# Save any remaining word block\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(block_of_word_obj) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\core.py:288\u001b[39m, in \u001b[36mDataCore._process_word\u001b[39m\u001b[34m(self, word, pos_text, context, word_context)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# Update co-occurrence information for valid tags\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags_to_discard:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_cooccurrence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_of_word_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindows_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# Generate keyword candidates involving this term\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[38;5;28mself\u001b[39m._generate_candidates((tag, word), term_obj, block_of_word_obj, n)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\core.py:312\u001b[39m, in \u001b[36mDataCore._update_cooccurrence\u001b[39m\u001b[34m(self, block_of_word_obj, term_obj, windows_size)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[33;03mUpdate co-occurrence information between terms.\u001b[39;00m\n\u001b[32m    301\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    308\u001b[39m \u001b[33;03m    windows_size (int): Size of co-occurrence window to consider\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;66;03m# Calculate the window of previous words to consider for co-occurrence\u001b[39;00m\n\u001b[32m    311\u001b[39m word_windows = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(block_of_word_obj) - windows_size), \u001b[38;5;28mlen\u001b[39m(block_of_word_obj))\n\u001b[32m    313\u001b[39m )\n\u001b[32m    315\u001b[39m \u001b[38;5;66;03m# For each word in the window, update co-occurrence if it's a valid term\u001b[39;00m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m word_windows:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yake\n",
    "\n",
    "# Initialiser l'extracteur\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "\n",
    "# Charger tous les fichiers\n",
    "data_path = \"../../data/TP4_Corpus/\"\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "\n",
    "# Dictionnaire global : {mot_clé : meilleur_score}\n",
    "global_keywords = {}\n",
    "\n",
    "for f in files:\n",
    "    text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read()\n",
    "\n",
    "    # Extraction des mots-clés du fichier\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "    for kw, score in keywords:\n",
    "        # Ne garder que les bigrammes\n",
    "        if len(kw.split()) == 2:\n",
    "            # YAKE : plus le score est petit, mieux c’est → on garde le meilleur\n",
    "            if kw in global_keywords:\n",
    "                global_keywords[kw] = min(global_keywords[kw], score)\n",
    "            else:\n",
    "                global_keywords[kw] = score\n",
    "\n",
    "# Trier les bigrammes globaux selon leur importance (score croissant)\n",
    "sorted_keywords = sorted(global_keywords.items(), key=lambda x: x[1])\n",
    "\n",
    "# Choisir le top N global (ex : top 50)\n",
    "top_N = 50\n",
    "top_global = sorted_keywords[:top_N]\n",
    "\n",
    "top_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0992a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     text = \u001b[38;5;28mopen\u001b[39m(os.path.join(data_path, f), \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m).read()\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Extraire les mots clés\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     keywords = \u001b[43mkw_extractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\core\\yake.py:213\u001b[39m, in \u001b[36mKeywordExtractor.extract_keywords\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    207\u001b[39m core_config = {\n\u001b[32m    208\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwindows_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mwindow_size\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    209\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    210\u001b[39m }\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Initialize the data core with the text\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m dc = \u001b[43mDataCore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopword_set\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstopword_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcore_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Build features for single terms and multi-word terms\u001b[39;00m\n\u001b[32m    216\u001b[39m dc.build_single_terms_features(features=\u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\core.py:86\u001b[39m, in \u001b[36mDataCore.__init__\u001b[39m\u001b[34m(self, text, stopword_set, config)\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mself\u001b[39m._state[\u001b[33m\"\u001b[39m\u001b[33mcollections\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mfreq_ns\u001b[39m\u001b[33m\"\u001b[39m][i + \u001b[32m1\u001b[39m] = \u001b[32m0.0\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Process the text and build all data structures\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindows_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\core.py:180\u001b[39m, in \u001b[36mDataCore._build\u001b[39m\u001b[34m(self, text, windows_size, n)\u001b[39m\n\u001b[32m    177\u001b[39m text = pre_filter(text)\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Split text into sentences and tokenize\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28mself\u001b[39m.sentences_str = \u001b[43mtokenize_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.number_of_sentences = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.sentences_str)\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Initialize position counter for global word positions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\utils.py:80\u001b[39m, in \u001b[36mtokenize_sentences\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize_sentences\u001b[39m(text):\n\u001b[32m     65\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03m    Split text into sentences and tokenize into words.\u001b[39;00m\n\u001b[32m     67\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33;03m              for a single sentence in the original text\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Inner list: tokenize each sentence into words\u001b[39;49;00m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m            \u001b[49m\u001b[43mw\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Keep only valid word tokens\u001b[39;49;00m\n\u001b[32m     84\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_contractions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweb_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Filter out standalone apostrophes and empty tokens\u001b[39;49;00m\n\u001b[32m     86\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Outer list: iterate through sentences\u001b[39;49;00m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_multi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Skip empty sentences\u001b[39;49;00m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\utils.py:84\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize_sentences\u001b[39m(text):\n\u001b[32m     65\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03m    Split text into sentences and tokenize into words.\u001b[39;00m\n\u001b[32m     67\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33;03m              for a single sentence in the original text\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m     81\u001b[39m         \u001b[38;5;66;03m# Inner list: tokenize each sentence into words\u001b[39;00m\n\u001b[32m     82\u001b[39m         [\n\u001b[32m     83\u001b[39m             w  \u001b[38;5;66;03m# Keep only valid word tokens\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m split_contractions(\u001b[43mweb_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     85\u001b[39m             \u001b[38;5;66;03m# Filter out standalone apostrophes and empty tokens\u001b[39;00m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (w.startswith(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(w) > \u001b[32m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(w) > \u001b[32m0\u001b[39m\n\u001b[32m     87\u001b[39m         ]\n\u001b[32m     88\u001b[39m         \u001b[38;5;66;03m# Outer list: iterate through sentences\u001b[39;00m\n\u001b[32m     89\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(split_multi(text))\n\u001b[32m     90\u001b[39m         \u001b[38;5;66;03m# Skip empty sentences\u001b[39;00m\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s.strip()) > \u001b[32m0\u001b[39m\n\u001b[32m     92\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\segtok\\tokenizer.py:306\u001b[39m, in \u001b[36mweb_tokenizer\u001b[39m\u001b[34m(sentence)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;129m@_matches\u001b[39m(\u001b[33mr\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[33m    (?<=^|[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms<\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m[\u001b[39m\u001b[33m{\u001b[39m\u001b[33m])            # visual border\u001b[39m\n\u001b[32m    282\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mweb_tokenizer\u001b[39m(sentence):\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    The web tokenizer works like the :func:`word_tokenizer`, but does not split URIs or\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[33;03m    e-mail addresses. It also un-escapes all escape sequences (except in URIs or email addresses).\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspan\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweb_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43munescape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\segtok\\tokenizer.py:307\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;129m@_matches\u001b[39m(\u001b[33mr\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[33m    (?<=^|[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms<\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m[\u001b[39m\u001b[33m{\u001b[39m\u001b[33m])            # visual border\u001b[39m\n\u001b[32m    282\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mweb_tokenizer\u001b[39m(sentence):\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    The web tokenizer works like the :func:`word_tokenizer`, but does not split URIs or\u001b[39;00m\n\u001b[32m    304\u001b[39m \u001b[33;03m    e-mail addresses. It also un-escapes all escape sequences (except in URIs or email addresses).\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [token \u001b[38;5;28;01mfor\u001b[39;00m i, span \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(web_tokenizer.split(sentence))\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m ((span,) \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mword_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43munescape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\segtok\\tokenizer.py:237\u001b[39m, in \u001b[36mword_tokenizer\u001b[39m\u001b[34m(sentence)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03mThis tokenizer extends the alphanumeric :func:`symbol_tokenizer` by splitting fewer cases:\u001b[39;00m\n\u001b[32m    213\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m \u001b[33;03m6. Subscript digits are attached if prefixed with letters that look like a chemical formula.\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    236\u001b[39m pruned = HYPHENATED_LINEBREAK.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m2\u001b[39m\u001b[33m'\u001b[39m, sentence)\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m tokens = \u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspan\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspace_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpruned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\n\u001b[32m    238\u001b[39m \u001b[43m          \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# splice the sentence terminal off the last word/token if it has any at its borders\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# only look for the sentence terminal in the last three tokens\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(tokens[-\u001b[32m3\u001b[39m:]), \u001b[32m1\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\segtok\\tokenizer.py:238\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03mThis tokenizer extends the alphanumeric :func:`symbol_tokenizer` by splitting fewer cases:\u001b[39;00m\n\u001b[32m    213\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m \u001b[33;03m6. Subscript digits are attached if prefixed with letters that look like a chemical formula.\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    236\u001b[39m pruned = HYPHENATED_LINEBREAK.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m2\u001b[39m\u001b[33m'\u001b[39m, sentence)\n\u001b[32m    237\u001b[39m tokens = [token \u001b[38;5;28;01mfor\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m space_tokenizer(pruned) \u001b[38;5;28;01mfor\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m           token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m token]\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# splice the sentence terminal off the last word/token if it has any at its borders\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# only look for the sentence terminal in the last three tokens\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(tokens[-\u001b[32m3\u001b[39m:]), \u001b[32m1\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import yake\n",
    "\n",
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor\n",
    "\n",
    "# Lister les fichiers du corpus complet\n",
    "data_path = \"../../data/TP4_Corpus/\"\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "\n",
    "# Boucle sur tous les fichiers du corpus\n",
    "for f in sorted(files):\n",
    "    text = open(os.path.join(data_path, f), 'r', encoding=\"utf-8\").read()\n",
    "\n",
    "# Extraire les mots clés\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c819b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne garder que les bigrammes\n",
    "    kept = []\n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 2:\n",
    "            kept.append(kw)\n",
    "\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept)}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
