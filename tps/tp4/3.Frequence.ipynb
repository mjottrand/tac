{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de la distribution du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports et dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer une une liste de stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6212429 words before tokenization\n",
      "8237776 words found\n"
     ]
    }
   ],
   "source": [
    "# Récupération du contenu du fichier\n",
    "path = \"../../data/all.txt\"\n",
    "limit = 10**8\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()[:limit]\n",
    "\n",
    "# Nombre de mots avant tokenization (approche simple : découpe sur les espaces)\n",
    "words_before = [w for w in text.split() if w.strip()]\n",
    "print(f\"{len(words_before)} words before tokenization\")\n",
    "\n",
    "# Tokenization\n",
    "words = nltk.wordpunct_tokenize(text)\n",
    "print(f\"{len(words)} words found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325 stopwords:\n",
      " ['actuellement', 'ai', 'aie', 'aient', 'aies', 'ailleurs', 'ainsi', 'ait', 'alors', 'année', 'années', 'ans', 'après', 'as', 'assez', 'au', 'aucun', 'aujourd', 'aura', 'aurai', 'auraient', 'aurais', 'aurait', 'auras', 'aurez', 'auriez', 'aurions', 'aurons', 'auront', 'aussi', 'autre', 'autres', 'aux', 'avaient', 'avais', 'avait', 'avant', 'avec', 'avez', 'aviez', 'avions', 'avoir', 'avons', 'avril', 'ayant', 'ayante', 'ayantes', 'ayants', 'ayez', 'ayons', 'bas', 'beaucoup', 'bien', 'bon', 'bonne', 'c', 'car', 'cas', 'ce', 'cela', 'celle', 'celui', 'cependant', 'certains', 'ces', 'cet', 'cette', 'ceux', 'chaque', 'chez', 'cinq', 'comme', 'compte', 'cours', 'd', 'dans', 'de', 'depuis', 'dernier', 'dernière', 'des', 'deux', 'devant', 'dimanche', 'dire', 'dit', 'doit', 'donc', 'donné', 'dont', 'du', 'dès', 'décembre', 'déjà', 'effet', 'elle', 'elles', 'en', 'encore', 'enfants', 'enfin', 'ensuite', 'entre', 'es', 'est', 'et', 'eu', 'eue', 'eues', 'eurent', 'eus', 'eusse', 'eussent', 'eusses', 'eussiez', 'eussions', 'eut', 'eux', 'eûmes', 'eût', 'eûtes', 'faire', 'fait', 'faut', 'fin', 'fois', 'fort', 'furent', 'fus', 'fusse', 'fussent', 'fusses', 'fussiez', 'fussions', 'fut', 'fûmes', 'fût', 'fûtes', 'grand', 'grande', 'générale', 'heure', 'heures', 'homme', 'hommes', 'hui', 'ici', 'il', 'ils', 'j', 'jamais', 'janvier', 'je', 'jeudi', 'jour', 'journal', 'jours', 'juillet', 'juin', 'jusqu', 'l', 'la', 'laquelle', 'le', 'les', 'leur', 'leurs', 'lieu', 'lui', 'lundi', 'm', 'ma', 'mais', 'maison', 'mardi', 'mars', 'matin', 'me', 'ment', 'mercredi', 'mes', 'midi', 'mis', 'moi', 'moins', 'mois', 'moment', 'mon', 'même', 'n', 'ne', 'nom', 'nombre', 'non', 'nord', 'nos', 'notamment', 'notre', 'nous', 'nouveau', 'nouvelle', 'novembre', 'octobre', 'on', 'ont', 'ou', 'par', 'parce', 'parmi', 'part', 'partie', 'pas', 'pendant', 'peu', 'peut', 'place', 'plus', 'plusieurs', 'point', 'porte', 'pour', 'pourrait', 'première', 'prendre', 'presse', 'pris', 'problème', 'près', 'puis', 'qu', 'quand', 'quatre', 'que', 'quelque', 'quelques', 'question', 'qui', 'raison', 'reste', 'rien', 'rue', 's', 'sa', 'saint', 'samedi', 'sans', 'se', 'selon', 'semaine', 'sera', 'serai', 'seraient', 'serais', 'serait', 'seras', 'serez', 'seriez', 'serions', 'serons', 'seront', 'ses', 'seulement', 'situation', 'soient', 'soir', 'sois', 'soit', 'sommes', 'son', 'sont', 'sous', 'soyez', 'soyons', 'suis', 'suite', 'sur', 't', 'ta', 'tant', 'te', 'temps', 'tes', 'toi', 'ton', 'toujours', 'tous', 'tout', 'toute', 'toutes', 'trois', 'trop', 'très', 'tu', 'un', 'une', 'van', 'vendredi', 'vers', 'vie', 'vient', 'ville', 'voir', 'vos', 'votre', 'vous', 'vue', 'y', 'à', 'également', 'étaient', 'étais', 'était', 'étant', 'étante', 'étantes', 'étants', 'étiez', 'étions', 'été', 'étée', 'étées', 'étés', 'êtes', 'être', 'œuvre']\n",
      "2737333 words kept (256502 different word forms)\n"
     ]
    }
   ],
   "source": [
    "# Stopwords (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"plus\", \"cette\", \"être\", \"tout\", \"fait\", \"comme\", \"leurs\", \"deux\",\n",
    "\"bien\", \"après\", \"sans\", \"dont\", \"tous\", \"encore\", \"faire\", \"peut\",\n",
    "\"aussi\", \"ceux\", \"elles\", \"alors\", \"toujours\", \"devant\",\n",
    "\"également\", \"aujourd\", \"dernier\", \"première\", \"nouvelle\", \"certains\",\n",
    "\"quatre\", \"trop\", \"dès\", \"quand\", \"notamment\", \"cependant\", \"jamais\",\n",
    "\"ici\", \"beaucoup\", \"ensuite\", \"assez\", \"puis\", \"laquelle\", \"chaque\",\n",
    "\"seulement\", \"entre\", \"sous\", \"dit\", \"autres\", \"très\", \"autre\",\n",
    "\"ans\", \"ainsi\", \"peu\", \"non\", \"depuis\", \"avoir\", \"moins\", \"toute\",\n",
    "\"trois\", \"toutes\", \"quelques\", \"faut\", \"cet\", \"celui\", \"doit\", \"jusqu\",\n",
    "\"vie\", \"déjà\", \"celle\", \"vers\", \"dire\", \"cela\", \"fois\", \"donc\",\n",
    "\"pendant\", \"année\", \"cours\", \"grande\", \"grand\", \"part\", \"rue\", \"avant\", \n",
    "\"mois\", \"van\", \"jour\", \"heures\", \"point\", \"situation\", \"question\", \"soir\", \n",
    "\"nouveau\", \"fin\", \"hui\", \"jours\", \"suite\", \"vue\", \"ville\", \"car\", \"moment\", \n",
    "\"place\", \"compte\", \"voir\", \"cas\", \"rien\", \"effet\", \"matin\", \"ailleurs\", \n",
    "\"plusieurs\", \"vient\", \"partie\", \"saint\", \"chez\", \"janvier\", \"près\", \n",
    "\"générale\", \"mardi\", \"dimanche\", \"lundi\", \"mars\", \"décembre\", \"octobre\", \n",
    "\"tant\", \"reste\", \"ment\", \"bon\", \"fort\", \"pris\", \"maison\", \"jeudi\", \"nom\", \n",
    "\"temps\", \"lieu\", \"homme\", \"problème\", \"hommes\", \"midi\", \"heure\", \"parce\",\n",
    "\"raison\", \"cinq\", \"nord\", \"œuvre\", \"années\", \"avril\", \"semaine\",\n",
    "\"pourrait\", \"juin\", \"selon\", \"novembre\", \"donné\", \"bas\", \"porte\", \"prendre\", \n",
    "\"quelque\", \"enfin\",  \"nombre\",\"actuellement\", \"dernière\", \"enfants\", \n",
    "\"vendredi\", \"mis\",\"aucun\",\"bonne\", \"presse\", \"mercredi\", \"parmi\",\"enfin\",  \n",
    "\"juillet\", \"samedi\", \"journal\",\"quelque\", \n",
    "]\n",
    "sw = set(sw)\n",
    "\n",
    "print(f\"{len(sw)} stopwords:\\n {sorted(sw)}\")\n",
    "\n",
    "# Eliminer les stopwords et les termes non alphabétiques\n",
    "kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "voc = set(kept)\n",
    "print(f\"{len(kept)} words kept ({len(voc)} different word forms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculer la taille du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2737333 words kept (256502 different word forms)\n"
     ]
    }
   ],
   "source": [
    "# Eliminer les stopwords et les termes non alphabétiques\n",
    "kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "voc = set(kept)\n",
    "print(f\"{len(kept)} words kept ({len(voc)} different word forms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences of 'austérité' before filtering: 1337\n",
      "3040929 words kept (256630 different word forms)\n",
      "Occurrences of 'austérité' after filtering: 1337\n",
      "Occurrences of 'austérité' with fuzzy matching: 1395\n",
      "Variants considered: ['ïaustérité', 'austérité', 'paustérité', 'aaustérité', 'faustérité', 'vaustérité', 'austcrité', 'austérités', 'austéri', 'austèrt']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from thefuzz import process\n",
    "\n",
    "# Stopwords (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"plus\", \"cette\", \"être\", \"tout\", \"fait\", \"comme\", \"leurs\", \"deux\",\n",
    "\"bien\", \"après\", \"sans\", \"dont\", \"tous\", \"encore\", \"faire\", \"peut\",\n",
    "\"aussi\", \"ceux\", \"elles\", \"alors\", \"toujours\", \"devant\",\n",
    "\"également\", \"aujourd\", \"dernier\", \"première\", \"nouvelle\", \"certains\",\n",
    "\"quatre\", \"trop\", \"dès\", \"quand\", \"notamment\", \"cependant\", \"jamais\",\n",
    "\"ici\", \"beaucoup\", \"ensuite\", \"assez\", \"puis\", \"laquelle\", \"chaque\",\n",
    "\"seulement\", \"entre\", \"sous\", \"dit\", \"autres\", \"très\", \"autre\",\n",
    "\"ans\", \"ainsi\", \"peu\", \"non\", \"depuis\", \"avoir\", \"moins\", \"toute\",\n",
    "\"trois\", \"toutes\", \"quelques\", \"faut\", \"cet\", \"celui\", \"doit\", \"jusqu\",\n",
    "\"vie\", \"déjà\", \"celle\", \"vers\", \"dire\", \"cela\", \"fois\", \"donc\",\n",
    "\"pendant\", \"année\", \"cours\", \"grande\", \"grand\", \"part\", \"rue\", \"avant\", \n",
    "\"mois\", \"van\", \"jour\", \"heures\", \"point\", \"situation\", \"question\", \"soir\", \n",
    "\"nouveau\", \"fin\", \"hui\", \"jours\", \"suite\", \"vue\", \"ville\", \"car\", \"moment\", \n",
    "\"place\", \"compte\", \"voir\", \"cas\", \"rien\", \"effet\", \"matin\", \"ailleurs\", \n",
    "\"plusieurs\", \"vient\", \"partie\", \"saint\", \"chez\", \"janvier\", \"près\", \n",
    "\"générale\", \"mardi\", \"dimanche\", \"lundi\", \"mars\", \"décembre\", \"octobre\", \n",
    "\"tant\", \"reste\", \"ment\", \"bon\", \"fort\", \"pris\", \"maison\", \"jeudi\", \"nom\", \n",
    "\"temps\", \"lieu\", \"homme\", \"problème\", \"hommes\", \"midi\", \"heure\", \"parce\",\n",
    "\"raison\", \"cinq\", \"nord\", \"œuvre\", \"années\", \"avril\", \"semaine\",\n",
    "\"pourrait\", \"juin\", \"selon\", \"novembre\", \"donné\", \"bas\", \"porte\", \"prendre\", \n",
    "\"quelque\", \"enfin\",  \"nombre\",\"actuellement\", \"dernière\", \"enfants\", \n",
    "\"vendredi\", \"mis\",\"aucun\",\"bonne\", \"presse\", \"mercredi\", \"parmi\",\"enfin\",  \n",
    "\"juillet\", \"samedi\", \"journal\",\"quelque\", \n",
    "]\n",
    "sw = set(sw)\n",
    "\n",
    "# --- Lecture et tokenization ---\n",
    "path = \"../../data/all.txt\"\n",
    "limit = 10**8\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()[:limit]\n",
    "\n",
    "words = nltk.wordpunct_tokenize(text)\n",
    "\n",
    "# --- Compte avant filtrage ---\n",
    "word = \"austérité\"\n",
    "count_before = sum(1 for w in words if w.lower() == word)\n",
    "print(f\"Occurrences of '{word}' before filtering: {count_before}\")\n",
    "\n",
    "# --- Suppression stopwords et filtrage ---\n",
    "kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "voc = set(kept)\n",
    "print(f\"{len(kept)} words kept ({len(voc)} different word forms)\")\n",
    "\n",
    "# --- Compte après filtrage (forme exacte) ---\n",
    "count_after = sum(1 for w in kept if w == word)\n",
    "print(f\"Occurrences of '{word}' after filtering: {count_after}\")\n",
    "\n",
    "# --- Fuzzy matching pour le mot-clé ---\n",
    "matches = process.extract(word, voc, limit=10)\n",
    "# Sélection des variantes (score >= 80)\n",
    "variants = [w for w, score in matches if score >= 80]\n",
    "\n",
    "# --- Compte total avec fuzzy matching ---\n",
    "count_fuzzy = sum(kept.count(v) for v in variants)\n",
    "print(f\"Occurrences of '{word}' with fuzzy matching: {count_fuzzy}\")\n",
    "print(f\"Variants considered: {variants}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer les mots les plus fréquents et en faire un plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pays', 8907),\n",
       " ('gouvernement', 8072),\n",
       " ('ministre', 6987),\n",
       " ('contre', 6757),\n",
       " ('bruxelles', 5725),\n",
       " ('politique', 5480),\n",
       " ('président', 5207),\n",
       " ('belgique', 5185),\n",
       " ('prix', 4980),\n",
       " ('guerre', 4630),\n",
       " ('général', 4505),\n",
       " ('premier', 4002),\n",
       " ('belge', 3817),\n",
       " ('conseil', 3789),\n",
       " ('etat', 3732),\n",
       " ('millions', 3721),\n",
       " ('france', 3628),\n",
       " ('paris', 3479),\n",
       " ('francs', 3389),\n",
       " ('parti', 3313),\n",
       " ('etats', 3246),\n",
       " ('travail', 3177),\n",
       " ('loi', 3039),\n",
       " ('monde', 2993),\n",
       " ('économique', 2947),\n",
       " ('unis', 2879),\n",
       " ('ordre', 2773),\n",
       " ('affaires', 2727),\n",
       " ('français', 2670),\n",
       " ('marché', 2652),\n",
       " ('plan', 2590),\n",
       " ('nationale', 2538),\n",
       " ('chambre', 2515),\n",
       " ('belges', 2511),\n",
       " ('europe', 2503),\n",
       " ('londres', 2474),\n",
       " ('projet', 2435),\n",
       " ('allemagne', 2421),\n",
       " ('commission', 2326),\n",
       " ('économie', 2307),\n",
       " ('société', 2288),\n",
       " ('roi', 2257),\n",
       " ('membres', 2218),\n",
       " ('accord', 2211),\n",
       " ('chef', 2192),\n",
       " ('demande', 2142),\n",
       " ('déclaré', 2136),\n",
       " ('union', 2124),\n",
       " ('droit', 2073),\n",
       " ('congo', 2005),\n",
       " ('conférence', 1965),\n",
       " ('action', 1956),\n",
       " ('rapport', 1955),\n",
       " ('comité', 1933),\n",
       " ('mesures', 1930),\n",
       " ('nouvelles', 1890),\n",
       " ('commerce', 1881),\n",
       " ('production', 1852),\n",
       " ('anvers', 1824),\n",
       " ('sujet', 1818),\n",
       " ('travaux', 1809),\n",
       " ('service', 1803),\n",
       " ('paix', 1790),\n",
       " ('milliards', 1780),\n",
       " ('état', 1757),\n",
       " ('national', 1724),\n",
       " ('peuple', 1721),\n",
       " ('liège', 1704),\n",
       " ('pouvoir', 1703),\n",
       " ('programme', 1703),\n",
       " ('assemblée', 1671),\n",
       " ('industrie', 1667),\n",
       " ('travailleurs', 1650),\n",
       " ('secrétaire', 1629),\n",
       " ('défense', 1594),\n",
       " ('séance', 1586),\n",
       " ('armée', 1578),\n",
       " ('bretagne', 1550),\n",
       " ('mesure', 1531),\n",
       " ('ministres', 1525),\n",
       " ('publique', 1522),\n",
       " ('mort', 1515),\n",
       " ('budget', 1514),\n",
       " ('militaire', 1514),\n",
       " ('cent', 1503),\n",
       " ('surtout', 1503),\n",
       " ('donner', 1501),\n",
       " ('doute', 1499),\n",
       " ('banque', 1493),\n",
       " ('grands', 1492),\n",
       " ('produits', 1490),\n",
       " ('sait', 1484),\n",
       " ('seul', 1476),\n",
       " ('six', 1468),\n",
       " ('réunion', 1461),\n",
       " ('etc', 1460),\n",
       " ('août', 1460),\n",
       " ('semble', 1453),\n",
       " ('ouvriers', 1448),\n",
       " ('britannique', 1445)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(kept)\n",
    "fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détecter les Hapax (mots qui n'apparaissent qu'une fois dans le corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['râpés',\n",
       " 'btiède',\n",
       " 'découvreuse',\n",
       " 'donneuses',\n",
       " 'conriture',\n",
       " 'avilla',\n",
       " 'ruckert',\n",
       " 'tthland',\n",
       " 'robeson',\n",
       " 'chantoi',\n",
       " 'beson',\n",
       " 'courteru',\n",
       " 'yperster',\n",
       " 'matto',\n",
       " 'prisonnicrs',\n",
       " 'ilqsïïiglsollilqslilillliaesogl',\n",
       " 'impolitesse',\n",
       " 'brimait',\n",
       " 'dérisoirement',\n",
       " 'bouffarde',\n",
       " 'déambulai',\n",
       " 'fripé',\n",
       " 'becken',\n",
       " 'fug',\n",
       " 'encourant',\n",
       " 'duivels',\n",
       " 'pcèle',\n",
       " 'avnuêiis',\n",
       " 'jleuron',\n",
       " 'philantropiqucs',\n",
       " 'célestine',\n",
       " 'récupératrice',\n",
       " 'stelleman',\n",
       " 'quiche',\n",
       " 'pépins',\n",
       " 'survins',\n",
       " 'pruduits',\n",
       " 'crevaient',\n",
       " 'animerait',\n",
       " 'grinçants',\n",
       " 'exubérants',\n",
       " 'hélait',\n",
       " 'fwent',\n",
       " 'frrt',\n",
       " 'niiec',\n",
       " 'grassouillet',\n",
       " 'componction',\n",
       " 'clavicules',\n",
       " 'détale',\n",
       " 'enocker',\n",
       " 'houtsiplou',\n",
       " 'jandrin',\n",
       " 'drenouille',\n",
       " 'cristallisons',\n",
       " 'pochas',\n",
       " 'aigrelet',\n",
       " 'stellemans',\n",
       " 'épanuoi',\n",
       " 'égosiller',\n",
       " 'mauviette',\n",
       " 'augmenterons',\n",
       " 'jnorclicus',\n",
       " 'arêté',\n",
       " 'mfhj',\n",
       " 'fciaby',\n",
       " 'galeribs',\n",
       " 'vaudcville',\n",
       " 'vaudsv',\n",
       " 'bsur',\n",
       " 'lolits',\n",
       " 'idemam',\n",
       " 'cavalieria',\n",
       " 'hamil',\n",
       " 'aqlon',\n",
       " 'varroy',\n",
       " 'schauîen',\n",
       " 'bîur',\n",
       " 'ïideau',\n",
       " 'compagn',\n",
       " 'pnte',\n",
       " 'spectocle',\n",
       " 'monon',\n",
       " 'hoensel',\n",
       " 'vocabie',\n",
       " 'fulminent',\n",
       " 'retter',\n",
       " 'actuellle',\n",
       " 'psysanneri',\n",
       " 'libsrté',\n",
       " 'rièrré',\n",
       " 'lequsl',\n",
       " 'nécessa',\n",
       " 'néfa',\n",
       " 'piofité',\n",
       " 'appicv',\n",
       " 'rénovatirn',\n",
       " 'iuppose',\n",
       " 'séancs',\n",
       " 'ebouti',\n",
       " 'salamalecs']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.hapaxes()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction mots clés et bigrammes sur le corpus \"kept\"\n",
    "\n",
    "Ici permet une analyse décontextualisée des mots-clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 mots clés extraits (56 mots uniques)\n",
      "Top 100 mots clés :\n",
      "etats : 14\n",
      "unis : 14\n",
      "ministre : 11\n",
      "affaires : 8\n",
      "conseil : 7\n",
      "président : 6\n",
      "millions : 5\n",
      "parti : 4\n",
      "secrétaire : 4\n",
      "gouvernement : 3\n",
      "francs : 3\n",
      "commerce : 3\n",
      "belgique : 3\n",
      "ministres : 3\n",
      "pays : 3\n",
      "etat : 3\n",
      "étrangères : 2\n",
      "économiques : 2\n",
      "nationale : 2\n",
      "milliards : 2\n",
      "belges : 2\n",
      "bretagne : 2\n",
      "chambre : 2\n",
      "économique : 2\n",
      "publique : 2\n",
      "france : 2\n",
      "bruxelles : 2\n",
      "général : 2\n",
      "europe : 2\n",
      "banque : 2\n",
      "guerre : 2\n",
      "défense : 1\n",
      "travaux : 1\n",
      "publics : 1\n",
      "instruction : 1\n",
      "communiste : 1\n",
      "ambassadeur : 1\n",
      "extérieur : 1\n",
      "marché : 1\n",
      "commun : 1\n",
      "chef : 1\n",
      "cabinet : 1\n",
      "santé : 1\n",
      "classes : 1\n",
      "socialiste : 1\n",
      "belge : 1\n",
      "conférence : 1\n",
      "américain : 1\n",
      "commission : 1\n",
      "administration : 1\n",
      "ancien : 1\n",
      "vice : 1\n",
      "société : 1\n",
      "politique : 1\n",
      "occidentale : 1\n",
      "militaire : 1\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "from collections import Counter\n",
    "\n",
    "# 'kept' contient déjà ton corpus nettoyé (liste de mots)\n",
    "# Exemple : kept = ['pays', 'gouvernement', 'ministre', ...]\n",
    "\n",
    "# Convertir kept en texte pour YAKE (il attend une chaîne)\n",
    "text_kept = \" \".join(kept)\n",
    "\n",
    "# Instancier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "\n",
    "# Extraire les mots clés depuis le texte reconstitué\n",
    "keywords = kw_extractor.extract_keywords(text_kept)\n",
    "\n",
    "# Compteur pour les mots clés\n",
    "keyword_counter = Counter()\n",
    "\n",
    "# Découper les mots clés multi-mots en mots individuels\n",
    "for kw, score in keywords:\n",
    "    words = kw.split()\n",
    "    keyword_counter.update(words)\n",
    "\n",
    "# Récupérer les 50 mots clés les plus fréquents\n",
    "top_keywords = keyword_counter.most_common(100)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"{sum(keyword_counter.values())} mots clés extraits ({len(keyword_counter)} mots uniques)\")\n",
    "print(\"Top 100 mots clés :\")\n",
    "for word, count in top_keywords:\n",
    "    print(f\"{word} : {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici permet une analyse décontextualisée des bigrammes\n",
    "attention, les mots dans le corpus kept sont ordonnées dans l'ordre dans lequel il apparaissent dans le texte, mais intgéralement nettoyé. Ce sont des des paires de mots suivi du nombre de fois où il apparaissent ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 bigrammes les plus fréquents dans le corpus :\n",
      "etats unis: 2646\n",
      "premier ministre: 1351\n",
      "affaires étrangères: 942\n",
      "new york: 877\n",
      "nations unies: 606\n",
      "ministre affaires: 598\n",
      "projet loi: 573\n",
      "parti communiste: 471\n",
      "millions francs: 471\n",
      "chemins fer: 458\n",
      "plan marshall: 443\n",
      "marché commun: 440\n",
      "secrétaire général: 430\n",
      "président conseil: 418\n",
      "millions dollars: 411\n",
      "classes moyennes: 400\n",
      "ministre finances: 367\n",
      "vice président: 366\n",
      "banque nationale: 356\n",
      "milliards francs: 354\n",
      "millions livres: 351\n",
      "commerce extérieur: 348\n",
      "défense nationale: 347\n",
      "affaires économiques: 345\n",
      "travaux publics: 325\n",
      "beaux arts: 316\n",
      "vis vis: 302\n",
      "classe ouvrière: 298\n",
      "lutte contre: 291\n",
      "europe occidentale: 281\n",
      "union soviétique: 278\n",
      "voix contre: 275\n",
      "instruction publique: 264\n",
      "secrétaire etat: 264\n",
      "matières premières: 259\n",
      "politique austérité: 257\n",
      "général gaulle: 255\n",
      "chemin fer: 251\n",
      "conseil sécurité: 251\n",
      "opinion publique: 251\n",
      "conseil ministres: 250\n",
      "jeunes gens: 250\n",
      "état major: 248\n",
      "francs belges: 244\n",
      "gouvernement français: 240\n",
      "conseil communal: 235\n",
      "parti socialiste: 235\n",
      "conseil administration: 230\n",
      "arrêté royal: 229\n",
      "chambre commerce: 228\n",
      "gouvernement belge: 227\n",
      "allemagne occidentale: 226\n",
      "sir stafford: 224\n",
      "ministre intérieur: 218\n",
      "sécurité sociale: 217\n",
      "politique économique: 217\n",
      "ministère affaires: 214\n",
      "directeur général: 213\n",
      "ancien ministre: 213\n",
      "jeune fille: 207\n",
      "président république: 207\n",
      "services publics: 206\n",
      "ministre défense: 200\n",
      "gouvernement britannique: 198\n",
      "hausse prix: 195\n",
      "chef cabinet: 194\n",
      "congo belge: 190\n",
      "drapeau rouge: 188\n",
      "prix revient: 179\n",
      "mesures prises: 179\n",
      "premiere page: 177\n",
      "président chambre: 176\n",
      "président truman: 176\n",
      "parti libéral: 172\n",
      "libre belgique: 172\n",
      "stafford cripps: 172\n",
      "pays européens: 168\n",
      "allocations familiales: 167\n",
      "pacte atlantique: 166\n",
      "loi unique: 166\n",
      "millions tonnes: 164\n",
      "monde entier: 163\n",
      "pouvoirs publics: 161\n",
      "lever coucher: 161\n",
      "comité national: 160\n",
      "assemblée nationale: 160\n",
      "santé publique: 160\n",
      "pouvoir achat: 158\n",
      "balance paiements: 158\n",
      "cour appel: 154\n",
      "sud ouest: 154\n",
      "ministre justice: 152\n",
      "ministre etat: 150\n",
      "dommages guerre: 150\n",
      "pays europe: 149\n",
      "dix huit: 148\n",
      "livre sterling: 147\n",
      "bourse bruxelles: 146\n",
      "envoyé spécial: 145\n",
      "anciens combattants: 144\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "# bigram_counter pour compter les paires de mots\n",
    "bigram_counter = Counter()\n",
    "\n",
    "# Générer tous les bigrammes de kept\n",
    "bigrams = ngrams(kept, 2)  # paires de mots consécutifs\n",
    "\n",
    "# Ajouter uniquement les bigrammes qui ne contiennent pas de stopwords\n",
    "# (ici sw = set des stopwords français)\n",
    "filtered_bigrams = [\n",
    "    (w1, w2) for w1, w2 in bigrams if w1 not in sw and w2 not in sw\n",
    "]\n",
    "\n",
    "# Mettre à jour le compteur\n",
    "bigram_counter.update(filtered_bigrams)\n",
    "\n",
    "# Récupérer les 100 bigrammes les plus fréquents\n",
    "top_100_bigrams = bigram_counter.most_common(100)\n",
    "\n",
    "# Affichage\n",
    "print(\"Top 100 bigrammes les plus fréquents dans le corpus :\")\n",
    "for (w1, w2), count in top_100_bigrams:\n",
    "    print(f\"{w1} {w2}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
