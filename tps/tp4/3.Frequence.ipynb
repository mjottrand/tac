{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de la distribution du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports et dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 stopwords:\n",
      " ['ai', 'aie', 'aient', 'aies', 'ainsi', 'ait', 'après', 'as', 'au', 'aura', 'aurai', 'auraient', 'aurais', 'aurait', 'auras', 'aurez', 'auriez', 'aurions', 'aurons', 'auront', 'aussi', 'autre', 'autres', 'aux', 'avaient', 'avais', 'avait', 'avec', 'avez', 'aviez', 'avions', 'avoir', 'avons', 'ayant', 'ayante', 'ayantes', 'ayants', 'ayez', 'ayons', 'bien', 'c', 'ce', 'cela', 'celle', 'ces', 'cet', 'cette', 'comme', 'contre', 'd', 'dans', 'de', 'depuis', 'des', 'deux', 'dire', 'dit', 'doit', 'donc', 'dont', 'du', 'elle', 'en', 'encore', 'entre', 'es', 'est', 'et', 'eu', 'eue', 'eues', 'eurent', 'eus', 'eusse', 'eussent', 'eusses', 'eussiez', 'eussions', 'eut', 'eux', 'eûmes', 'eût', 'eûtes', 'faire', 'fait', 'faut', 'furent', 'fus', 'fusse', 'fussent', 'fusses', 'fussiez', 'fussions', 'fut', 'fûmes', 'fût', 'fûtes', 'het', 'il', 'ils', 'j', 'je', 'jusqu', 'l', 'la', 'le', 'les', 'leur', 'lui', 'm', 'ma', 'mais', 'me', 'mes', 'moi', 'moins', 'mon', 'même', 'n', 'ne', 'non', 'nos', 'notre', 'nous', 'on', 'ont', 'ou', 'par', 'pas', 'pendant', 'peut', 'plus', 'pour', 'qu', 'que', 'qui', 's', 'sa', 'sans', 'se', 'sera', 'serai', 'seraient', 'serais', 'serait', 'seras', 'serez', 'seriez', 'serions', 'serons', 'seront', 'ses', 'soient', 'sois', 'soit', 'sommes', 'son', 'sont', 'sous', 'soyez', 'soyons', 'suis', 'sur', 't', 'ta', 'te', 'tes', 'toi', 'ton', 'tous', 'tout', 'toutes', 'trois', 'tu', 'un', 'une', 'van', 'vos', 'votre', 'vous', 'y', 'à', 'étaient', 'étais', 'était', 'étant', 'étante', 'étantes', 'étants', 'étiez', 'étions', 'été', 'étée', 'étées', 'étés', 'êtes', 'être']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\", \n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\"]\n",
    "sw = set(sw)\n",
    "\n",
    "print(f\"{len(sw)} stopwords:\\n {sorted(sw)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer une une liste de stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Récupération du contenu du fichier\n",
    "path = \"../../data/all.txt\"\n",
    "limit = 10**8\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8237776 words found\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "words = nltk.wordpunct_tokenize(text)\n",
    "print(f\"{len(words)} words found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculer la taille du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3040929 words kept (256630 different word forms)\n"
     ]
    }
   ],
   "source": [
    "# Eliminer les stopwords et les termes non alphabétiques\n",
    "kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "voc = set(kept)\n",
    "print(f\"{len(kept)} words kept ({len(voc)} different word forms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer les mots les plus fréquents et en faire un plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pays', 8907),\n",
       " ('gouvernement', 8072),\n",
       " ('ministre', 6987),\n",
       " ('bruxelles', 5725),\n",
       " ('leurs', 5656),\n",
       " ('politique', 5480),\n",
       " ('président', 5207),\n",
       " ('belgique', 5185),\n",
       " ('très', 5011),\n",
       " ('prix', 4980),\n",
       " ('cours', 4862),\n",
       " ('guerre', 4630),\n",
       " ('ans', 4577),\n",
       " ('grande', 4507),\n",
       " ('général', 4505),\n",
       " ('peu', 4347),\n",
       " ('temps', 4141),\n",
       " ('grand', 4086),\n",
       " ('part', 4070),\n",
       " ('rue', 4040),\n",
       " ('premier', 4002),\n",
       " ('avant', 3932),\n",
       " ('belge', 3817),\n",
       " ('conseil', 3789),\n",
       " ('toute', 3789),\n",
       " ('mois', 3782),\n",
       " ('etat', 3732),\n",
       " ('millions', 3721),\n",
       " ('jour', 3669),\n",
       " ('france', 3628),\n",
       " ('quelques', 3572),\n",
       " ('paris', 3479),\n",
       " ('ceux', 3462),\n",
       " ('francs', 3389),\n",
       " ('heures', 3328),\n",
       " ('parti', 3313),\n",
       " ('point', 3269),\n",
       " ('situation', 3265),\n",
       " ('celui', 3265),\n",
       " ('question', 3262),\n",
       " ('etats', 3246),\n",
       " ('lieu', 3228),\n",
       " ('travail', 3177),\n",
       " ('soir', 3144),\n",
       " ('vie', 3106),\n",
       " ('déjà', 3088),\n",
       " ('vers', 3044),\n",
       " ('loi', 3039),\n",
       " ('monde', 2993),\n",
       " ('elles', 2977),\n",
       " ('économique', 2947),\n",
       " ('unis', 2879),\n",
       " ('fois', 2873),\n",
       " ('alors', 2867),\n",
       " ('toujours', 2850),\n",
       " ('nouveau', 2820),\n",
       " ('devant', 2779),\n",
       " ('ordre', 2773),\n",
       " ('nouvelle', 2751),\n",
       " ('année', 2731),\n",
       " ('affaires', 2727),\n",
       " ('fin', 2723),\n",
       " ('hui', 2697),\n",
       " ('jours', 2692),\n",
       " ('première', 2678),\n",
       " ('français', 2670),\n",
       " ('marché', 2652),\n",
       " ('suite', 2645),\n",
       " ('vue', 2594),\n",
       " ('plan', 2590),\n",
       " ('également', 2548),\n",
       " ('nationale', 2538),\n",
       " ('chambre', 2515),\n",
       " ('belges', 2511),\n",
       " ('aujourd', 2511),\n",
       " ('dernier', 2503),\n",
       " ('europe', 2503),\n",
       " ('ville', 2497),\n",
       " ('londres', 2474),\n",
       " ('car', 2471),\n",
       " ('projet', 2435),\n",
       " ('allemagne', 2421),\n",
       " ('moment', 2417),\n",
       " ('place', 2398),\n",
       " ('compte', 2376),\n",
       " ('voir', 2373),\n",
       " ('cas', 2373),\n",
       " ('rien', 2360),\n",
       " ('effet', 2348),\n",
       " ('commission', 2326),\n",
       " ('matin', 2312),\n",
       " ('économie', 2307),\n",
       " ('ailleurs', 2301),\n",
       " ('société', 2288),\n",
       " ('plusieurs', 2284),\n",
       " ('vient', 2260),\n",
       " ('partie', 2257),\n",
       " ('roi', 2257),\n",
       " ('saint', 2232),\n",
       " ('chez', 2220)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(kept)\n",
    "fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détecter les Hapax (mots qui n'apparaissent qu'une fois dans le corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['râpés',\n",
       " 'btiède',\n",
       " 'découvreuse',\n",
       " 'donneuses',\n",
       " 'conriture',\n",
       " 'avilla',\n",
       " 'ruckert',\n",
       " 'tthland',\n",
       " 'robeson',\n",
       " 'chantoi',\n",
       " 'beson',\n",
       " 'courteru',\n",
       " 'yperster',\n",
       " 'matto',\n",
       " 'prisonnicrs',\n",
       " 'ilqsïïiglsollilqslilillliaesogl',\n",
       " 'impolitesse',\n",
       " 'brimait',\n",
       " 'dérisoirement',\n",
       " 'bouffarde',\n",
       " 'déambulai',\n",
       " 'fripé',\n",
       " 'becken',\n",
       " 'fug',\n",
       " 'encourant',\n",
       " 'duivels',\n",
       " 'pcèle',\n",
       " 'avnuêiis',\n",
       " 'jleuron',\n",
       " 'philantropiqucs',\n",
       " 'célestine',\n",
       " 'récupératrice',\n",
       " 'stelleman',\n",
       " 'quiche',\n",
       " 'pépins',\n",
       " 'survins',\n",
       " 'pruduits',\n",
       " 'crevaient',\n",
       " 'animerait',\n",
       " 'grinçants',\n",
       " 'exubérants',\n",
       " 'hélait',\n",
       " 'fwent',\n",
       " 'frrt',\n",
       " 'niiec',\n",
       " 'grassouillet',\n",
       " 'componction',\n",
       " 'clavicules',\n",
       " 'détale',\n",
       " 'enocker',\n",
       " 'houtsiplou',\n",
       " 'jandrin',\n",
       " 'drenouille',\n",
       " 'cristallisons',\n",
       " 'pochas',\n",
       " 'aigrelet',\n",
       " 'stellemans',\n",
       " 'épanuoi',\n",
       " 'égosiller',\n",
       " 'mauviette',\n",
       " 'augmenterons',\n",
       " 'jnorclicus',\n",
       " 'arêté',\n",
       " 'mfhj',\n",
       " 'fciaby',\n",
       " 'galeribs',\n",
       " 'vaudcville',\n",
       " 'vaudsv',\n",
       " 'bsur',\n",
       " 'lolits',\n",
       " 'idemam',\n",
       " 'cavalieria',\n",
       " 'hamil',\n",
       " 'aqlon',\n",
       " 'varroy',\n",
       " 'schauîen',\n",
       " 'bîur',\n",
       " 'ïideau',\n",
       " 'compagn',\n",
       " 'pnte',\n",
       " 'spectocle',\n",
       " 'monon',\n",
       " 'hoensel',\n",
       " 'vocabie',\n",
       " 'fulminent',\n",
       " 'retter',\n",
       " 'actuellle',\n",
       " 'psysanneri',\n",
       " 'libsrté',\n",
       " 'rièrré',\n",
       " 'lequsl',\n",
       " 'nécessa',\n",
       " 'néfa',\n",
       " 'piofité',\n",
       " 'appicv',\n",
       " 'rénovatirn',\n",
       " 'iuppose',\n",
       " 'séancs',\n",
       " 'ebouti',\n",
       " 'salamalecs']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.hapaxes()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction mots clés et bigrammes sur le corpus \"kept\"\n",
    "\n",
    "Ici permet une analyse décontextualisée des mots-clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 mots clés extraits (68 mots uniques)\n",
      "Top 100 mots clés :\n",
      "ministre : 11\n",
      "etats : 11\n",
      "unis : 11\n",
      "grande : 6\n",
      "bretagne : 6\n",
      "affaires : 5\n",
      "pays : 5\n",
      "président : 4\n",
      "france : 4\n",
      "bruxelles : 4\n",
      "francs : 3\n",
      "millions : 3\n",
      "belgique : 3\n",
      "commerce : 3\n",
      "parti : 3\n",
      "économique : 3\n",
      "ans : 3\n",
      "conseil : 3\n",
      "étrangères : 2\n",
      "économiques : 2\n",
      "belges : 2\n",
      "cours : 2\n",
      "société : 2\n",
      "générale : 2\n",
      "chambre : 2\n",
      "publique : 2\n",
      "secrétaire : 2\n",
      "défense : 1\n",
      "nationale : 1\n",
      "gouvernement : 1\n",
      "milliards : 1\n",
      "travaux : 1\n",
      "publics : 1\n",
      "dernières : 1\n",
      "années : 1\n",
      "voir : 1\n",
      "suite : 1\n",
      "page : 1\n",
      "conférence : 1\n",
      "presse : 1\n",
      "communiste : 1\n",
      "instruction : 1\n",
      "rue : 1\n",
      "royale : 1\n",
      "point : 1\n",
      "vue : 1\n",
      "ambassadeur : 1\n",
      "ministres : 1\n",
      "extérieur : 1\n",
      "marché : 1\n",
      "commun : 1\n",
      "situation : 1\n",
      "bas : 1\n",
      "cour : 1\n",
      "appel : 1\n",
      "général : 1\n",
      "chef : 1\n",
      "cabinet : 1\n",
      "etat : 1\n",
      "santé : 1\n",
      "banque : 1\n",
      "classes : 1\n",
      "socialiste : 1\n",
      "belge : 1\n",
      "europe : 1\n",
      "guerre : 1\n",
      "commission : 1\n",
      "administration : 1\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "from collections import Counter\n",
    "\n",
    "# 'kept' contient déjà ton corpus nettoyé (liste de mots)\n",
    "# Exemple : kept = ['pays', 'gouvernement', 'ministre', ...]\n",
    "\n",
    "# Convertir kept en texte pour YAKE (il attend une chaîne)\n",
    "text_kept = \" \".join(kept)\n",
    "\n",
    "# Instancier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "\n",
    "# Extraire les mots clés depuis le texte reconstitué\n",
    "keywords = kw_extractor.extract_keywords(text_kept)\n",
    "\n",
    "# Compteur pour les mots clés\n",
    "keyword_counter = Counter()\n",
    "\n",
    "# Découper les mots clés multi-mots en mots individuels\n",
    "for kw, score in keywords:\n",
    "    words = kw.split()\n",
    "    keyword_counter.update(words)\n",
    "\n",
    "# Récupérer les 50 mots clés les plus fréquents\n",
    "top_keywords = keyword_counter.most_common(100)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"{sum(keyword_counter.values())} mots clés extraits ({len(keyword_counter)} mots uniques)\")\n",
    "print(\"Top 100 mots clés :\")\n",
    "for word, count in top_keywords:\n",
    "    print(f\"{word} : {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici permet une analyse décontextualisée des bigrammes\n",
    "attention, les mots dans le corpus kept sont ordonnées dans l'ordre dans lequel il apparaissent dans le texte, mais intgéralement nettoyé. Ce sont des des paires de mots suivi du nombre de fois où il apparaissent ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 bigrammes les plus fréquents dans le corpus :\n",
      "etats unis: 2646\n",
      "aujourd hui: 2493\n",
      "grande bretagne: 1479\n",
      "premier ministre: 1351\n",
      "affaires étrangères: 942\n",
      "new york: 877\n",
      "point vue: 775\n",
      "nations unies: 606\n",
      "pays bas: 604\n",
      "ministre affaires: 597\n",
      "projet loi: 573\n",
      "ordre jour: 555\n",
      "parti communiste: 471\n",
      "millions francs: 471\n",
      "chemins fer: 458\n",
      "plan marshall: 443\n",
      "avant guerre: 440\n",
      "marché commun: 440\n",
      "secrétaire général: 430\n",
      "président conseil: 417\n",
      "millions dollars: 411\n",
      "assemblée générale: 403\n",
      "classes moyennes: 400\n",
      "porte parole: 388\n",
      "quelques jours: 373\n",
      "première fois: 368\n",
      "vice président: 366\n",
      "ministre finances: 366\n",
      "dès lors: 360\n",
      "banque nationale: 356\n",
      "milliards francs: 354\n",
      "millions livres: 351\n",
      "commerce extérieur: 348\n",
      "défense nationale: 347\n",
      "affaires économiques: 345\n",
      "main œuvre: 336\n",
      "travaux publics: 325\n",
      "beaux arts: 316\n",
      "suite page: 312\n",
      "grand nombre: 303\n",
      "vis vis: 302\n",
      "classe ouvrière: 298\n",
      "quelque chose: 298\n",
      "peu près: 294\n",
      "certain nombre: 289\n",
      "europe occidentale: 281\n",
      "union soviétique: 278\n",
      "instruction publique: 264\n",
      "secrétaire etat: 264\n",
      "matières premières: 259\n",
      "politique austérité: 257\n",
      "général gaulle: 255\n",
      "grande partie: 253\n",
      "chemin fer: 251\n",
      "conseil sécurité: 251\n",
      "opinion publique: 251\n",
      "jeunes gens: 250\n",
      "conseil ministres: 249\n",
      "état major: 248\n",
      "dernières années: 245\n",
      "francs belges: 244\n",
      "gouvernement français: 240\n",
      "conseil communal: 235\n",
      "parti socialiste: 235\n",
      "quelque peu: 234\n",
      "chaque jour: 233\n",
      "conseil administration: 230\n",
      "arrêté royal: 229\n",
      "chambre commerce: 228\n",
      "allemagne occidentale: 226\n",
      "gouvernement belge: 226\n",
      "sir stafford: 224\n",
      "fin année: 220\n",
      "quelque temps: 219\n",
      "ministre intérieur: 218\n",
      "sécurité sociale: 217\n",
      "politique économique: 217\n",
      "cinq ans: 215\n",
      "hôtel ville: 214\n",
      "ministère affaires: 214\n",
      "directeur général: 213\n",
      "ancien ministre: 211\n",
      "jeune fille: 207\n",
      "président république: 207\n",
      "services publics: 206\n",
      "six mois: 206\n",
      "coût vie: 205\n",
      "mercredi matin: 204\n",
      "dix ans: 204\n",
      "année dernière: 203\n",
      "semaine dernière: 203\n",
      "première page: 200\n",
      "ministre défense: 200\n",
      "gouvernement britannique: 198\n",
      "âgé ans: 197\n",
      "rue royale: 197\n",
      "chef cabinet: 194\n",
      "voir suite: 193\n",
      "chaque année: 192\n",
      "quelques mois: 191\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "\n",
    "# bigram_counter pour compter les paires de mots\n",
    "bigram_counter = Counter()\n",
    "\n",
    "# Générer tous les bigrammes de kept\n",
    "bigrams = ngrams(kept, 2)  # paires de mots consécutifs\n",
    "\n",
    "# Ajouter uniquement les bigrammes qui ne contiennent pas de stopwords\n",
    "# (ici sw = set des stopwords français)\n",
    "filtered_bigrams = [\n",
    "    (w1, w2) for w1, w2 in bigrams if w1 not in sw and w2 not in sw\n",
    "]\n",
    "\n",
    "# Mettre à jour le compteur\n",
    "bigram_counter.update(filtered_bigrams)\n",
    "\n",
    "# Récupérer les 100 bigrammes les plus fréquents\n",
    "top_100_bigrams = bigram_counter.most_common(100)\n",
    "\n",
    "# Affichage\n",
    "print(\"Top 100 bigrammes les plus fréquents dans le corpus :\")\n",
    "for (w1, w2), count in top_100_bigrams:\n",
    "    print(f\"{w1} {w2}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
