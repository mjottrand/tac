{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction de Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraire les mots clés d'un document avec Yake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopwords (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"qu'il\",\"d'une\", \"d'un\", \"faire\",\"d’une\",\"d’un\", \"qu’il\", \"c'est\", \"c’est\", \"fut\", \"ans\", \"qu'ils\", \"déjà\", \"qu'on\" ]\n",
    "sw = set(sw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     text = f.read()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Extraire les mots clés de ce texte  ← (TES LIGNES)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m keywords = \u001b[43mkw_extractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m keywords  \u001b[38;5;66;03m# ← si tu veux l'afficher dans un notebook\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Ajouter uniquement les mots (pas les scores) dans le compteur global\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\core\\yake.py:213\u001b[39m, in \u001b[36mKeywordExtractor.extract_keywords\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    207\u001b[39m core_config = {\n\u001b[32m    208\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwindows_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mwindow_size\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    209\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    210\u001b[39m }\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Initialize the data core with the text\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m dc = \u001b[43mDataCore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopword_set\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstopword_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcore_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Build features for single terms and multi-word terms\u001b[39;00m\n\u001b[32m    216\u001b[39m dc.build_single_terms_features(features=\u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\core.py:86\u001b[39m, in \u001b[36mDataCore.__init__\u001b[39m\u001b[34m(self, text, stopword_set, config)\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mself\u001b[39m._state[\u001b[33m\"\u001b[39m\u001b[33mcollections\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mfreq_ns\u001b[39m\u001b[33m\"\u001b[39m][i + \u001b[32m1\u001b[39m] = \u001b[32m0.0\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Process the text and build all data structures\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindows_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\core.py:180\u001b[39m, in \u001b[36mDataCore._build\u001b[39m\u001b[34m(self, text, windows_size, n)\u001b[39m\n\u001b[32m    177\u001b[39m text = pre_filter(text)\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Split text into sentences and tokenize\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28mself\u001b[39m.sentences_str = \u001b[43mtokenize_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.number_of_sentences = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.sentences_str)\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Initialize position counter for global word positions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\yake\\data\\utils.py:89\u001b[39m, in \u001b[36mtokenize_sentences\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize_sentences\u001b[39m(text):\n\u001b[32m     65\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03m    Split text into sentences and tokenize into words.\u001b[39;00m\n\u001b[32m     67\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33;03m              for a single sentence in the original text\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m     81\u001b[39m         \u001b[38;5;66;03m# Inner list: tokenize each sentence into words\u001b[39;00m\n\u001b[32m     82\u001b[39m         [\n\u001b[32m     83\u001b[39m             w  \u001b[38;5;66;03m# Keep only valid word tokens\u001b[39;00m\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m split_contractions(web_tokenizer(s))\n\u001b[32m     85\u001b[39m             \u001b[38;5;66;03m# Filter out standalone apostrophes and empty tokens\u001b[39;00m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (w.startswith(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(w) > \u001b[32m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(w) > \u001b[32m0\u001b[39m\n\u001b[32m     87\u001b[39m         ]\n\u001b[32m     88\u001b[39m         \u001b[38;5;66;03m# Outer list: iterate through sentences\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(split_multi(text))\n\u001b[32m     90\u001b[39m         \u001b[38;5;66;03m# Skip empty sentences\u001b[39;00m\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s.strip()) > \u001b[32m0\u001b[39m\n\u001b[32m     92\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\segtok\\segmenter.py:259\u001b[39m, in \u001b[36m_sentences\u001b[39m\u001b[34m(spans, join_on_lowercase, short_sentence_length)\u001b[39m\n\u001b[32m    256\u001b[39m last = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    257\u001b[39m shorterThanATypicalSentence = \u001b[38;5;28;01mlambda\u001b[39;00m c, l: c < short_sentence_length \u001b[38;5;129;01mor\u001b[39;00m l < short_sentence_length\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_abbreviation_joiner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlast\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin_on_lowercase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mBEFORE_LOWER\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mLOWER_WORD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\segtok\\segmenter.py:301\u001b[39m, in \u001b[36m_abbreviation_joiner\u001b[39m\u001b[34m(spans)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prev_s[-\u001b[32m1\u001b[39m:].isspace():\n\u001b[32m    300\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m \u001b[38;5;66;03m# join\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m marker[\u001b[32m0\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mABBREVIATIONS\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_s\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m \u001b[38;5;66;03m# join\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m marker[\u001b[32m0\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m next_s \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    304\u001b[39m         LONE_WORD.match(next_s) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    305\u001b[39m         (ENDS_IN_DATE_DIGITS.search(prev_s) \u001b[38;5;129;01mand\u001b[39;00m MONTH.match(next_s)) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m    306\u001b[39m         (MIDDLE_INITIAL_END.search(prev_s) \u001b[38;5;129;01mand\u001b[39;00m UPPER_WORD_START.match(next_s))\n\u001b[32m    307\u001b[39m         ):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yake\n",
    "from collections import Counter\n",
    "\n",
    "# Instancier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "\n",
    "# Lister les fichiers .txt\n",
    "data_path = \"../../data/txt/\"\n",
    "files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "\n",
    "# Compteur global de mots clés\n",
    "keyword_counter = Counter()\n",
    "\n",
    "# Parcourir tous les fichiers\n",
    "for this_file in files:\n",
    "    file_path = os.path.join(data_path, this_file)\n",
    "\n",
    "    # Lire le texte du fichier\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Extraire les mots clés de ce texte  ← (TES LIGNES)\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    keywords  # ← si tu veux l'afficher dans un notebook\n",
    "\n",
    "    # Ajouter uniquement les mots (pas les scores) dans le compteur global\n",
    "    keyword_counter.update([kw for kw, score in keywords])\n",
    "\n",
    "# Récupérer les 100 mots clés les plus fréquents dans le corpus\n",
    "top_100_keywords = keyword_counter.most_common(100)\n",
    "\n",
    "top_100_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "987"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimer le nombre de fichiers identifiés\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 bigrammes les plus fréquents dans le corpus :\n",
      "Affaires étrangères: 75\n",
      "JOURNAL QUOTIDIEN: 50\n",
      "New York: 40\n",
      "Banque Nationale: 39\n",
      "Marché commun: 36\n",
      "Affaires économiques: 35\n",
      "parti communiste: 32\n",
      "gouvernement belge: 32\n",
      "gouvernement français: 29\n",
      "Nations Unies: 27\n",
      "plan Marshall: 27\n",
      "d'autre part: 24\n",
      "Parti communiste: 24\n",
      "qu'il faut: 23\n",
      "Van Acker: 22\n",
      "francs belges: 22\n",
      "gouvernement britannique: 22\n",
      "président Truman: 21\n",
      "Société Générale: 21\n",
      "Soir Illustré: 21\n",
      "parti socialiste: 19\n",
      "politique économique: 19\n",
      "d’autre part: 19\n",
      "Libre Belgique: 18\n",
      "PARTI COMMUNISTE: 16\n",
      "RUE ROYALE: 16\n",
      "D’autre part: 16\n",
      "loi unique: 15\n",
      "Congo belge: 15\n",
      "Stafford Cripps: 15\n",
      "Sir Stafford: 15\n",
      "CHEQUES POSTAUX: 15\n",
      "c'est qu'il: 14\n",
      "Congo Belge: 14\n",
      "question royale: 14\n",
      "LIBRE BELGIQUE: 14\n",
      "classe ouvrière: 13\n",
      "Plan Marshall: 13\n",
      "cours d'une: 13\n",
      "président Eisenhower: 12\n",
      "gouvernement soviétique: 12\n",
      "DRAPEAU ROUGE: 12\n",
      "Van den: 12\n",
      "Classes moyennes: 12\n",
      "Conseil communal: 12\n",
      "classes moyennes: 11\n",
      "D'autre part: 11\n",
      "gouvernement travailliste: 11\n",
      "Van der: 11\n",
      "mercredi matin: 11\n",
      "qu’il faut: 11\n",
      "Van Houtte: 10\n",
      "gouvernement congolais: 10\n",
      "secrétaire général: 10\n",
      "Nations unies: 10\n",
      "Conseil général: 10\n",
      "président Kennedy: 10\n",
      "jeune homme: 10\n",
      "qu'il n'est: 10\n",
      "Défense nationale: 10\n",
      "Travaux publics: 10\n",
      "rue Neuve: 10\n",
      "nouveau gouvernement: 10\n",
      "van Zeeland: 9\n",
      "d'un gouvernement: 9\n",
      "Foreign Office: 9\n",
      "roi Léopold: 9\n",
      "parts sociales: 9\n",
      "vendredi soir: 9\n",
      "président Johnson: 9\n",
      "pays européens: 8\n",
      "PLAN MARSHALL: 8\n",
      "partis communistes: 8\n",
      "jeune femme: 8\n",
      "Léopold III: 8\n",
      "troupes belges: 8\n",
      "Grand Prix: 8\n",
      "Commerce extérieur: 8\n",
      "Parti socialiste: 8\n",
      "Haute Autorité: 8\n",
      "roi Albert: 8\n",
      "Pacte Atlantique: 8\n",
      "jeune fille: 8\n",
      "partis politiques: 8\n",
      "mardi soir: 8\n",
      "arrêté royal: 8\n",
      "rue Royale: 8\n",
      "Conseil provincial: 8\n",
      "POSTAUX PUBLICITE: 8\n",
      "cours d’une: 8\n",
      "franc français: 8\n",
      "d'autres pays: 7\n",
      "parti travailliste: 7\n",
      "Vanden Boeynants: 7\n",
      "Nation Belge: 7\n",
      "gouvernement américain: 7\n",
      "Etats arabes: 7\n",
      "présent avis: 7\n",
      "Conseil économique: 7\n",
      "directeur général: 7\n"
     ]
    }
   ],
   "source": [
    "# Compteur global pour les bigrammes\n",
    "bigram_counter = Counter()\n",
    "\n",
    "# Parcourir tous les fichiers\n",
    "for f in sorted(files):  # tous les fichiers\n",
    "    file_path = os.path.join(data_path, f)\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Extraire les mots clés\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "    # Ne garder que les bigrammes et filtrer les stopwords\n",
    "        bigrams = [kw for kw, score in keywords \n",
    "        if len(kw.split()) == 2 and not any(word.lower() in sw for word in kw.split())\n",
    "    ]\n",
    "\n",
    "    # Ajouter au compteur global\n",
    "    bigram_counter.update(bigrams)\n",
    "\n",
    "# Récupérer les 100 bigrammes les plus fréquents dans tout le corpus\n",
    "top_100_bigrams = bigram_counter.most_common(100)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Top 100 bigrammes les plus fréquents dans le corpus :\")\n",
    "for kw, count in top_100_bigrams:\n",
    "    print(f\"{kw}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
